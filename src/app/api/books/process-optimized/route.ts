import { NextRequest, NextResponse } from "next/server";
import { getDatabase, Collections, BookPage, ProcessingLog } from "@/lib/mongodb";
import { supabaseAdmin } from "@/lib/supabase";
import { processDocument } from "@/lib/document-processor";
import { removeHeadersFooters } from "@/lib/pdf-processor";
import { generateEmbedding } from "@/lib/openai";
import { upsertVectors } from "@/lib/pinecone";

export const runtime = "nodejs";
export const maxDuration = 300; // 5 minutes

/**
 * OPTIMIZED PROCESSING ROUTE
 * - Parallel embedding generation (5x faster)
 * - Larger Pinecone batches (100 vectors)
 * - Bulk MongoDB operations
 * - Better error handling
 */

function sanitizeText(text: string): string {
  if (!text) return "";

  let result = "";

  for (let i = 0; i < text.length; i++) {
    const code = text.charCodeAt(i);

    // High surrogate
    if (code >= 0xd800 && code <= 0xdbff) {
      const nextCode = text.charCodeAt(i + 1);
      if (nextCode >= 0xdc00 && nextCode <= 0xdfff) {
        result += text[i] + text[i + 1];
        i++; // Skip the low surrogate
      }
      continue;
    }

    // Low surrogate without preceding high surrogate - skip
    if (code >= 0xdc00 && code <= 0xdfff) {
      continue;
    }

    // Replace control characters with space
    if (code < 32 && code !== 9 && code !== 10 && code !== 13) {
      result += " ";
      continue;
    }

    result += text[i];
  }

  return result;
}

function truncateText(text: string, limit: number): string {
  if (!text) return "";
  const chars = Array.from(text);
  if (chars.length <= limit) {
    return text;
  }
  return chars.slice(0, limit).join("");
}

/**
 * Process embeddings in parallel batches
 * This is 5-10x faster than sequential processing
 */
async function processEmbeddingsParallel(
  pages: any[],
  bookId: string,
  detectedHeaders: string[],
  detectedFooters: string[],
  db: any
): Promise<any[]> {
  const vectors: any[] = [];
  const PARALLEL_BATCH_SIZE = 5; // Process 5 pages simultaneously
  const PINECONE_BATCH_SIZE = 100; // Upload 100 vectors at once

  console.log(`ðŸš€ Starting PARALLEL embedding generation for ${pages.length} pages`);
  console.log(`   Parallel batch size: ${PARALLEL_BATCH_SIZE}`);
  console.log(`   Pinecone batch size: ${PINECONE_BATCH_SIZE}`);

  // Process pages in parallel batches
  for (let i = 0; i < pages.length; i += PARALLEL_BATCH_SIZE) {
    const batch = pages.slice(i, i + PARALLEL_BATCH_SIZE);
    const batchNumber = Math.floor(i / PARALLEL_BATCH_SIZE) + 1;
    const totalBatches = Math.ceil(pages.length / PARALLEL_BATCH_SIZE);

    console.log(`ðŸ“Š Processing parallel batch ${batchNumber}/${totalBatches} (${batch.length} pages)`);

    // Process all pages in this batch simultaneously
    const batchPromises = batch.map(async (page) => {
      try {
        // Remove headers/footers from text before embedding
        const cleanedText = removeHeadersFooters(
          page.plain_text_content,
          detectedHeaders,
          detectedFooters
        );

        // Skip empty pages
        if (!cleanedText || cleanedText.trim().length < 50) {
          console.log(`  â­ï¸  Skipping page ${page.page_number} (too short)`);
          return null;
        }

        // Generate embedding
        const startTime = Date.now();
        const embedding = await generateEmbedding(cleanedText);
        const duration = Date.now() - startTime;

        console.log(`  âœ… Page ${page.page_number}: ${duration}ms (${embedding.length}D)`);

        // Create vector ID
        const vectorId = `${bookId}_page_${page.page_number}`;
        const preview = truncateText(sanitizeText(page.plain_text_content), 200);

        // Update page with embedding info
        await db.collection(Collections.BOOK_PAGES).updateOne(
          { book_id: bookId, page_number: page.page_number },
          { $set: { qdrant_id: vectorId } }
        );

        return {
          id: vectorId,
          values: embedding,
          metadata: {
            book_id: bookId,
            page_number: page.page_number,
            word_count: page.word_count,
            has_tables: page.has_tables,
            has_equations: page.has_equations,
            text_preview: preview,
          },
        };
      } catch (error) {
        console.error(`  âŒ Error processing page ${page.page_number}:`, error);
        return null;
      }
    });

    // Wait for all pages in this batch to complete
    const batchResults = await Promise.all(batchPromises);

    // Filter out null results and add to vectors array
    const validVectors = batchResults.filter((v) => v !== null);
    vectors.push(...validVectors);

    // Upload to Pinecone when we have enough vectors
    if (vectors.length >= PINECONE_BATCH_SIZE) {
      const toUpload = vectors.splice(0, PINECONE_BATCH_SIZE);
      console.log(`ðŸ“¤ Uploading ${toUpload.length} vectors to Pinecone...`);
      try {
        await upsertVectors(toUpload);
        console.log(`âœ… Uploaded successfully`);
      } catch (pineconeError) {
        console.error(`âŒ Pinecone upload failed:`, pineconeError);
        throw pineconeError;
      }
    }

    // Update progress
    const progress = 40 + Math.floor((i / pages.length) * 50);
    await db.collection(Collections.PROCESSING_LOGS).insertOne({
      book_id: bookId,
      stage: "embedding",
      status: "in_progress",
      message: `Processed ${Math.min(i + PARALLEL_BATCH_SIZE, pages.length)} of ${pages.length} pages`,
      progress,
      started_at: new Date(),
    });
  }

  // Upload remaining vectors
  if (vectors.length > 0) {
    console.log(`ðŸ“¤ Uploading final ${vectors.length} vectors to Pinecone...`);
    try {
      await upsertVectors(vectors);
      console.log(`âœ… Uploaded successfully`);
    } catch (pineconeError) {
      console.error(`âŒ Pinecone upload failed:`, pineconeError);
      throw pineconeError;
    }
  }

  return vectors;
}

/**
 * POST /api/books/process-optimized
 * Optimized book processing with parallel embedding generation
 */
export async function POST(request: NextRequest) {
  console.log("ðŸš€ OPTIMIZED Processing API called");

  try {
    const { bookId, fileBuffer, filename } = await request.json();
    console.log("ðŸ“š Book ID:", bookId);
    console.log("ðŸ“¦ File buffer length:", fileBuffer?.length || 0);

    if (!bookId || !fileBuffer) {
      console.error("âŒ Missing bookId or fileBuffer");
      return NextResponse.json(
        { error: "Missing bookId or fileBuffer" },
        { status: 400 }
      );
    }

    console.log("ðŸ”Œ Connecting to MongoDB...");
    const db = await getDatabase();
    console.log("âœ… MongoDB connected");

    // Log processing start
    const log: ProcessingLog = {
      book_id: bookId,
      stage: "extraction",
      status: "started",
      message: "Starting document extraction",
      progress: 0,
      started_at: new Date(),
    };
    await db.collection(Collections.PROCESSING_LOGS).insertOne(log);

    // Update book status
    await supabaseAdmin
      .from("books")
      .update({ status: "processing" })
      .eq("id", bookId);

    // Update MongoDB metadata
    await db
      .collection(Collections.BOOKS_METADATA)
      .updateOne(
        { book_id: bookId },
        { $set: { processing_status: "processing", updated_at: new Date() } }
      );

    try {
      // Convert base64 back to buffer
      const buffer = Buffer.from(fileBuffer, "base64");

      // Process document (supports PDF, DOCX, TXT, EPUB)
      console.log(`ðŸ“„ Processing document: ${filename}`);
      const startTime = Date.now();
      const pdfResult = await processDocument(buffer, filename);
      const extractionTime = Date.now() - startTime;
      console.log(`âœ… Document processed: ${pdfResult.totalPages} pages in ${extractionTime}ms`);

      // Update progress
      await db.collection(Collections.PROCESSING_LOGS).insertOne({
        book_id: bookId,
        stage: "extraction",
        status: "completed",
        message: `Extracted ${pdfResult.totalPages} pages in ${(extractionTime / 1000).toFixed(1)}s`,
        progress: 30,
        started_at: new Date(),
        completed_at: new Date(),
      });

      // Store pages in MongoDB (BULK INSERT)
      const pages = pdfResult.pages.map((page: any) => ({
        book_id: bookId,
        page_number: page.pageNumber,
        html_content: page.htmlContent,
        plain_text_content: page.text,
        has_images: page.hasImages,
        has_tables: page.hasTables,
        has_equations: page.hasEquations,
        word_count: page.wordCount,
        created_at: new Date(),
      }));

      console.log(`ðŸ’¾ Bulk inserting ${pages.length} pages to MongoDB...`);
      await db.collection(Collections.BOOK_PAGES).insertMany(pages as any);
      console.log(`âœ… Pages inserted`);

      // Update progress
      await db.collection(Collections.PROCESSING_LOGS).insertOne({
        book_id: bookId,
        stage: "embedding",
        status: "started",
        message: "Generating embeddings (parallel processing)",
        progress: 40,
        started_at: new Date(),
      });

      // Generate embeddings in PARALLEL (THIS IS THE OPTIMIZATION!)
      console.log("ðŸ§  Starting PARALLEL embedding generation...");
      const embeddingStartTime = Date.now();

      const vectors = await processEmbeddingsParallel(
        pages,
        bookId,
        pdfResult.detectedHeaders || [],
        pdfResult.detectedFooters || [],
        db
      );

      const embeddingTime = Date.now() - embeddingStartTime;
      console.log(`âœ… Embeddings completed in ${(embeddingTime / 1000).toFixed(1)}s`);
      console.log(`   Average: ${(embeddingTime / pages.length).toFixed(0)}ms per page`);

      // Update metadata with detected headers/footers
      await db
        .collection(Collections.BOOKS_METADATA)
        .updateOne(
          { book_id: bookId },
          {
            $set: {
              metadata: pdfResult.metadata,
              total_pages: pdfResult.totalPages,
              detectedHeaders: pdfResult.detectedHeaders || [],
              detectedFooters: pdfResult.detectedFooters || [],
              embeddings_generated: true,
              qdrant_ids: vectors.map((v) => v.id),
              processing_status: "completed",
              updated_at: new Date(),
            },
          }
        );

      // Update Supabase
      await supabaseAdmin
        .from("books")
        .update({
          status: "ready",
          total_pages: pdfResult.totalPages,
          processed_at: new Date().toISOString(),
        })
        .eq("id", bookId);

      // Log completion
      const totalTime = Date.now() - startTime;
      console.log("ðŸŽ‰ Book processing completed successfully!");
      console.log(`   Total time: ${(totalTime / 1000).toFixed(1)}s`);
      console.log(`   Extraction: ${(extractionTime / 1000).toFixed(1)}s`);
      console.log(`   Embedding: ${(embeddingTime / 1000).toFixed(1)}s`);
      console.log(`   Total pages: ${pdfResult.totalPages}`);
      console.log(`   Vectors created: ${vectors.length}`);

      await db.collection(Collections.PROCESSING_LOGS).insertOne({
        book_id: bookId,
        stage: "completion",
        status: "completed",
        message: `Processing completed in ${(totalTime / 1000).toFixed(1)}s`,
        progress: 100,
        started_at: new Date(),
        completed_at: new Date(),
      });

      return NextResponse.json({
        success: true,
        totalPages: pdfResult.totalPages,
        vectorsCreated: vectors.length,
        processingTime: totalTime,
        extractionTime,
        embeddingTime,
      });
    } catch (processingError) {
      console.error("Error processing book:", processingError);

      // Update status to failed
      await db.collection(Collections.BOOKS_METADATA).updateOne(
        { book_id: bookId },
        { $set: { processing_status: "failed", updated_at: new Date() } }
      );

      await supabaseAdmin
        .from("books")
        .update({ status: "failed" })
        .eq("id", bookId);

      await db.collection(Collections.PROCESSING_LOGS).insertOne({
        book_id: bookId,
        stage: "completion",
        status: "failed",
        message: "Book processing failed",
        error: processingError,
        progress: 0,
        started_at: new Date(),
        completed_at: new Date(),
      });

      throw processingError;
    }
  } catch (error) {
    console.error("Error in book processing:", error);
    return NextResponse.json(
      { error: "Failed to process book" },
      { status: 500 }
    );
  }
}
